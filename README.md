# 🚀 LLM Fine-tuning Project with Indian Datasets

A comprehensive fine-tuning project demonstrating 5 different techniques on Indian domain-specific datasets with complete API deployment, Docker containerization, and Kubernetes orchestration.

## 📊 Fine-tuning Techniques

| Technique | Dataset | Domain | Model Type | Use Case |
|-----------|---------|--------|------------|----------|
| **Full Fine-tuning** | HR Dataset | Employee policies, leave management | Complete parameter training | HR queries, company policies |
| **DPO (Direct Preference Optimization)** | Finance Dataset | Banking, investments, GST | Preference-based training | Financial advice, tax queries |
| **PEFT** | Sales Dataset | Retail, e-commerce, customer service | Parameter-efficient training | Sales strategies, customer support |
| **LoRA** | Healthcare Dataset | Medical queries, Ayurveda | Low-rank adaptation | Medical advice, health queries |
| **QLoRA** | Marketing Dataset | Campaigns, regional strategies | Quantized LoRA | Marketing strategies, campaigns |

## 🏗️ Project Structure

```
finetuningfinal/
├── 📁 datasets/                    # Indian domain datasets
│   ├── hr_dataset.json            # HR policies & leave management
│   ├── finance_dpo_dataset.json   # Banking & investment queries
│   ├── sales_dataset.json         # E-commerce & customer service
│   ├── healthcare_dataset.json     # Medical & Ayurveda queries
│   └── marketing_dataset.json      # Campaign strategies
├── 📁 finetuning/                  # Training scripts
│   ├── full_finetuning.py         # HR model (complete training)
│   ├── dpo_finetuning.py          # Finance model (preference optimization)
│   ├── peft_finetuning.py         # Sales model (parameter-efficient)
│   ├── lora_finetuning.py         # Healthcare model (low-rank adaptation)
│   └── qlora_finetuning.py        # Marketing model (quantized LoRA)
├── 📁 models/                      # Trained model checkpoints
│   ├── hr_full_finetuned/          # HR model outputs
│   ├── finance_dpo_finetuned/     # Finance model outputs
│   ├── sales_peft_finetuned/      # Sales model outputs
│   ├── healthcare_lora_finetuned/ # Healthcare model outputs
│   └── marketing_qlora_finetuned/# Marketing model outputs
├── 🐳 Docker & Deployment
│   ├── Dockerfile                  # Docker configuration
│   ├── docker-compose.yml         # Docker Compose setup
│   ├── kubernetes.yaml            # Kubernetes deployment
│   └── api_server.py              # FastAPI server
├── 🚀 Training Scripts
│   ├── train_all_models.py         # Master training script
│   ├── train_all.bat              # Windows batch file
│   └── train_all.sh               # Linux/H100 script
└── 📋 Configuration
    ├── requirements.txt            # Core dependencies
    ├── requirements_api.txt        # API dependencies
    └── README.md                  # This file
```

## 🛠️ Installation

### Prerequisites
- Python 3.10+
- CUDA-compatible GPU (8GB+ VRAM recommended)
- Docker (for containerization)
- Kubernetes cluster (for orchestration)

### Install Dependencies
```bash
# Core dependencies
pip install -r requirements.txt

# API dependencies
pip install -r requirements_api.txt
```

## 🎯 Training

### Quick Start - Train All Models
```bash
# Option 1: Python script (recommended)
python train_all_models.py

# Option 2: Windows batch file
train_all.bat

# Option 3: Linux/H100 script
./train_all.sh
```

### Individual Model Training
```bash
# HR Model (Full Fine-tuning)
python finetuning/full_finetuning.py

# Finance Model (DPO)
python finetuning/dpo_finetuning.py

# Sales Model (PEFT)
python finetuning/peft_finetuning.py

# Healthcare Model (LoRA)
python finetuning/lora_finetuning.py

# Marketing Model (QLoRA)
python finetuning/qlora_finetuning.py
```

### H100 GPU Training
```bash
# Set environment variables for H100 optimization
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Run training
./h100_training_commands.sh
```

## 🌐 API Deployment

### Local API Server
```bash
# Start API server
python api_server.py

# API will be available at: http://localhost:8000
# Interactive docs: http://localhost:8000/docs
```

### API Endpoints

| Endpoint | Method | Description | Example |
|----------|--------|-------------|---------|
| `/health` | GET | Health check | `curl http://localhost:8000/health` |
| `/api/models` | GET | List available models | `curl http://localhost:8000/api/models` |
| `/api/hr` | POST | HR model inference | See examples below |
| `/api/healthcare` | POST | Healthcare model inference | See examples below |
| `/api/marketing` | POST | Marketing model inference | See examples below |
| `/api/sales` | POST | Sales model inference | See examples below |
| `/api/finance` | POST | Finance model inference | See examples below |

### API Usage Examples

#### HR Model Query
```bash
curl -X POST http://localhost:8000/api/hr \
  -H "Content-Type: application/json" \
  -d '{
    "query": "How do I apply for casual leave?",
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

#### Healthcare Model Query
```bash
curl -X POST http://localhost:8000/api/healthcare \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the symptoms of dengue fever?",
    "max_tokens": 150
  }'
```

#### Marketing Model Query
```bash
curl -X POST http://localhost:8000/api/marketing \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Create a Diwali marketing campaign",
    "max_tokens": 200,
    "temperature": 0.8
  }'
```

## 🐳 Docker Deployment

### Build Docker Image
```bash
# Build the Docker image
docker build -t llm-api:latest .

# For H100 GPU optimization
docker build -t llm-api-h100:latest .
```

### Run with Docker Compose
```bash
# Start all services
docker-compose up -d

# Check logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Manual Docker Run
```bash
# CPU-only deployment
docker run -d \
  --name llm-api \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/datasets:/app/datasets \
  llm-api:latest

# GPU-enabled deployment (requires NVIDIA Docker)
docker run -d \
  --name llm-api-gpu \
  --gpus all \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/datasets:/app/datasets \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 \
  llm-api:latest
```

### Docker Commands
```bash
# Check container status
docker ps

# View logs
docker logs llm-api

# Access container shell
docker exec -it llm-api bash

# Stop container
docker stop llm-api

# Remove container
docker rm llm-api
```

## ☸️ Kubernetes Deployment

### Prerequisites
- Kubernetes cluster with GPU nodes
- NVIDIA GPU Operator installed
- kubectl configured

### Deploy to Kubernetes
```bash
# Apply Kubernetes configuration
kubectl apply -f kubernetes.yaml

# Check deployment status
kubectl get pods
kubectl get services

# View logs
kubectl logs -f deployment/llm-api

# Get service URL
kubectl get service llm-api-service
```

### Kubernetes Configuration
The `kubernetes.yaml` includes:
- **Deployment**: 3 replicas with GPU resource requests
- **Service**: LoadBalancer for external access
- **ConfigMap**: Environment variables
- **PersistentVolume**: Model storage
- **Resource Limits**: GPU and memory constraints

### Scaling
```bash
# Scale up replicas
kubectl scale deployment llm-api --replicas=5

# Check resource usage
kubectl top pods
kubectl top nodes
```

## 📊 Performance Benchmarks

### Training Performance
| Model Type | Training Time | Memory Usage | Model Size |
|------------|---------------|--------------|------------|
| Full Fine-tuning | 45-60 min | 12-16GB VRAM | 2.2GB |
| LoRA | 20-30 min | 6-8GB VRAM | 50MB |
| PEFT | 25-35 min | 8-10GB VRAM | 100MB |
| QLoRA | 15-25 min | 4-6GB VRAM | 25MB |
| DPO | 30-40 min | 10-12GB VRAM | 2.2GB |

### Inference Performance
| Model | Load Time | First Query | Subsequent | Tokens/sec |
|-------|-----------|-------------|------------|------------|
| HR (Full) | 5-10s | 2-3s | 0.5-1s | 100-150 |
| Healthcare (LoRA) | 3-5s | 1-2s | 0.3-0.5s | 150-200 |
| Sales (PEFT) | 3-5s | 1-2s | 0.3-0.5s | 150-200 |
| Marketing (QLoRA) | 2-3s | 0.5-1s | 0.2-0.3s | 200-250 |
| Finance (DPO) | 5-10s | 2-3s | 0.5-1s | 100-150 |

## 🔧 Configuration

### Environment Variables
```bash
# GPU Configuration
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export TOKENIZERS_PARALLELISM=false

# API Configuration
export API_HOST=0.0.0.0
export API_PORT=8000
export MAX_TOKENS=200
export TEMPERATURE=0.7
```

### Model Configuration
Each model can be configured with:
- **max_tokens**: Maximum response length (50-500)
- **temperature**: Creativity level (0.1-1.0)
- **top_p**: Nucleus sampling (0.1-1.0)
- **repetition_penalty**: Avoid repetition (1.0-1.5)

## 🧪 Testing

### Health Check
```bash
curl http://localhost:8000/health
```

### Model Testing
```bash
# Test all models
python -c "
import requests
import json

# Test HR model
response = requests.post('http://localhost:8000/api/hr', 
  json={'query': 'How to apply for leave?', 'max_tokens': 100})
print('HR Model:', response.json()['response'][:100])

# Test Healthcare model  
response = requests.post('http://localhost:8000/api/healthcare',
  json={'query': 'Symptoms of fever?', 'max_tokens': 100})
print('Healthcare Model:', response.json()['response'][:100])
"
```

## 📈 Monitoring

### Docker Monitoring
```bash
# Container stats
docker stats llm-api

# Resource usage
docker exec llm-api nvidia-smi
```

### Kubernetes Monitoring
```bash
# Pod metrics
kubectl top pods

# Node metrics
kubectl top nodes

# Resource usage
kubectl describe pod <pod-name>
```

## 🚨 Troubleshooting

### Common Issues

#### GPU Not Detected
```bash
# Check NVIDIA driver
nvidia-smi

# Test Docker GPU support
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

#### Out of Memory
```bash
# Reduce batch size in training scripts
# Use gradient accumulation
# Enable gradient checkpointing
```

#### API Connection Issues
```bash
# Check if API is running
curl http://localhost:8000/health

# Check Docker logs
docker logs llm-api

# Check Kubernetes logs
kubectl logs -f deployment/llm-api
```

## 📚 Dataset Information

| Dataset | Size | Domain | Language | Examples |
|---------|------|--------|----------|----------|
| HR | 500+ samples | Employee policies, leave management | Hindi/English | "How to apply for casual leave?" |
| Finance | 500+ pairs | Banking, investments, GST | Hindi/English | "What is the current GST rate?" |
| Sales | 500+ samples | E-commerce, customer service | Hindi/English | "How to handle customer complaints?" |
| Healthcare | 500+ samples | Medical queries, Ayurveda | Hindi/English | "What are symptoms of dengue?" |
| Marketing | 500+ samples | Campaigns, strategies | Hindi/English | "Create a Diwali campaign" |

## 🎯 Use Cases

### Business Applications
- **HR Assistant**: Employee policy queries, leave management
- **Financial Advisor**: Investment advice, tax calculations
- **Sales Support**: Customer service, product recommendations
- **Medical Assistant**: Symptom analysis, health advice
- **Marketing Manager**: Campaign creation, strategy planning

### Integration Examples
- **Chatbots**: Customer service automation
- **Mobile Apps**: Personal assistants
- **Websites**: Interactive help systems
- **APIs**: Third-party integrations

## 🔒 Security Considerations

### API Security
- Rate limiting (10 requests/minute per IP)
- Authentication tokens
- HTTPS/SSL encryption
- Input validation and sanitization

### Model Security
- Model encryption
- Access control
- Audit logging
- Data privacy compliance

## 📞 Support

### Getting Help
- Check logs: `docker logs llm-api`
- Health check: `curl http://localhost:8000/health`
- API docs: `http://localhost:8000/docs`

### Performance Optimization
- Use GPU acceleration
- Enable model caching
- Implement request batching
- Monitor resource usage

---

## 🎉 Quick Start Summary

1. **Install**: `pip install -r requirements.txt`
2. **Train**: `python train_all_models.py`
3. **Deploy**: `docker-compose up -d`
4. **Test**: `curl http://localhost:8000/health`
5. **Use**: Visit `http://localhost:8000/docs`

**Your LLM fine-tuning project is ready for production! 🚀**

## Create Python Virtual Environment
- **Install `uv`:**

   ```bash
   pip install uv

   uv --version
- **Create a Virtual Environmen:**
    ```bash
    uv venv .venv --python=3.10
- **Active Virtual Environment:**
    ```bash
    .venv\Scripts\activate
- **Initialize the `uv` Project:**
    ```bash
    uv init
- **Add Dependencies (e.g. add `python-dotenv`):**
    ```bash
    uv add python-dotenv

## Run:
- Streamlit app:
```
uv run streamlit run main.py --server.runOnSave true

- FastAPI app:
uvicorn main:app --reload
```
